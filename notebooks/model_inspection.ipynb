{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import joblib\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Matpolotlib image settings\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions\n",
    "\n",
    "def run_query(query):\n",
    "    \"\"\"Function that takes in a query, runs it on KCMO-MC Database and returns the results in a dataframe\"\"\"\n",
    "\n",
    "    engine = create_engine(\"postgresql:///kcmo-mc\")\n",
    "    db_conn = engine.connect()\n",
    "\n",
    "    with db_conn.begin():\n",
    "        df = pd.read_sql(query, db_conn)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_model_info(model_id):\n",
    "    \"\"\" \n",
    "    Takes in a Model ID and returns a df with information about the model including model set id,\n",
    "    model type, hyperparameters used in the model, train and validation start and end dates, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = f\"\"\"SELECT * from modeling.model_metadata\n",
    "            LEFT JOIN  modeling.model_set_metadata \n",
    "            USING (model_set_id)\n",
    "            WHERE model_id = '{model_id}'\n",
    "            ;\"\"\"\n",
    "\n",
    "    df = run_query(query)\n",
    "    return df.iloc[0,:]  # Returns only first row from dataframe - all rows should be duplicates except for run id\n",
    "\n",
    "\n",
    "\n",
    "def get_model_desc(model_id):\n",
    "    \"\"\" \n",
    "    Function that takes in a Model ID and returns a brief model description, \n",
    "    including model type, model hyperparameters, vailidation end date and model id\n",
    "    \"\"\"\n",
    "    \n",
    "    model_info = get_model_info(model_id)\n",
    "    model_desc = f\"\"\"{model_info['model_type']}\n",
    "        {model_info['model_hyperparams']}\n",
    "        validation end date: {model_info['val_end_date']}\n",
    "        model_id: {model_id}\"\"\"\n",
    "    return model_desc\n",
    "\n",
    "\n",
    "\n",
    "def get_model_predictions(model_id):\n",
    "    \"\"\" \n",
    "    Takes in Model ID and returns a DataFrame with the model predictions that includes\n",
    "    person_id, as_of_date, score, true label and rank\n",
    "    \"\"\"\n",
    "    query = f\"\"\"SELECT * from modeling.model_predictions\n",
    "            WHERE model_id = '{model_id}'\n",
    "            ;\"\"\"\n",
    "    df = run_query(query)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_feature_importances(model_id):\n",
    "    \"\"\" \n",
    "    Takes in Model ID and returns a DataFrame with feature impoortances for that model\n",
    "    \"\"\"\n",
    "    query = f\"\"\"Select * from modeling.feature_importances\n",
    "        WHERE model_id = '{model_id}'\n",
    "        ;\"\"\"\n",
    "    df = run_query(query)\n",
    "    return df\n",
    "\n",
    "\n",
    "def predictions_by_cats(model_id, crosstab_features):\n",
    "    \"\"\" Takes in a Model ID and list of features of interest for crosstabs (can also be only the first words / start of the features' names).\n",
    "    Returns crosstab_data, a DataFrame that includes the values of these features and model predictions \n",
    "    for each person_id; and column_names: a list that includes the column names for the features of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    #Load matrix from file\n",
    "    model_info = get_model_info(model_id)\n",
    "    matrix = pd.read_csv(model_info['val_matrix_path'])\n",
    "    #load predictions\n",
    "    predictions = get_model_predictions(model_id)\n",
    "    \n",
    "    #get column names for all features\n",
    "    column_names = []\n",
    "    for f in crosstab_features:\n",
    "        column = [col for col in matrix.columns if col.startswith(f)]\n",
    "        column_names = column_names + column\n",
    "\n",
    "    # join predictions and selected features\n",
    "    crosstab_data = predictions.set_index('person_id').join(matrix[column_names + ['person_id']].set_index('person_id'))\n",
    "\n",
    "    return crosstab_data, column_names\n",
    "\n",
    "\n",
    "\n",
    "def crosstab_at_k(crosstab_data, column_names, perc = 10):\n",
    "    \"\"\"Function that takes in a matrix \n",
    "\n",
    "    Args:\n",
    "        crosstab_data (DataFrame): A matrix including predictions, true labels, and features\n",
    "        column_names (list): Column names for the features of interest for crosstabs\n",
    "        perc (int, optional): The percentage of population to be labeled as 1 for crosstab calculations. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Descriptives of the data according to the features listed in column_names. \n",
    "    \"\"\"\n",
    "\n",
    "    # assign top perc label 1 based on rank\n",
    "    num_labeled_pos = np.ceil(perc * crosstab_data.shape[0]/100)\n",
    "    crosstab_data['pred_label'] = (crosstab_data['rank'] < num_labeled_pos).astype(int)\n",
    "    # assign signal detection vars\n",
    "    crosstab_data['tp'] = ((crosstab_data['pred_label'] == 1) & (crosstab_data['pred_label'] == crosstab_data['true_label'])).astype(int)\n",
    "    crosstab_data['fp'] = ((crosstab_data['pred_label'] == 1) & (crosstab_data['pred_label'] != crosstab_data['true_label'])).astype(int)\n",
    "    crosstab_data['tn'] = ((crosstab_data['pred_label'] == 0) & (crosstab_data['pred_label'] == crosstab_data['true_label'])).astype(int)\n",
    "    crosstab_data['fn'] = ((crosstab_data['pred_label'] == 0) & (crosstab_data['pred_label'] != crosstab_data['true_label'])).astype(int)\n",
    "\n",
    "    crosstabs = pd.DataFrame()\n",
    "    for column in column_names:\n",
    "        col_df = pd.DataFrame()\n",
    "        col_df['variable'] = [column]\n",
    "        col_df['base_rate'] = crosstab_data[column].mean()\n",
    "        col_df['top_k'] = (crosstab_data.loc[crosstab_data['pred_label'] == 1, column]).mean()\n",
    "        for v in ['tp', 'fp', 'tn', 'fn', 'true_label']:\n",
    "            mean_val = (crosstab_data.loc[crosstab_data[v] == 1, column]).mean()\n",
    "            col_df[v] = [mean_val]\n",
    "        crosstabs = pd.concat([crosstabs, col_df])\n",
    "\n",
    "    return crosstabs\n",
    "    \n",
    "\n",
    "\n",
    "def plot_score_dist(model_id, plt_type = 1):  \n",
    "    \"\"\" Takes in a Model ID and prints out the score distributions\"\"\"\n",
    "    # read in scores for the selected model\n",
    "    model_predictions = get_model_predictions(model_id)[['score', 'true_label']]\n",
    "    #plot score distribution according to true label\n",
    "    if plt_type == 2:\n",
    "        sns.displot(data = model_predictions, x ='score', hue = 'true_label', kind=\"kde\", common_norm = False, bw_method='scott')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1)\n",
    "        sns.histplot(data = model_predictions, x ='score', hue = 'true_label', stat='probability', binwidth = .05, kde = True, ax = ax)\n",
    "        ax.set_title(get_model_desc(model_id))\n",
    "\n",
    "\n",
    "\n",
    "def plot_top_features(model_id, num_features_to_plot = 20): \n",
    "    \"\"\" Takes in a Model ID and number of to features to plot out (defaults to 20)\n",
    "    \"\"\"\n",
    "    # read in feature importances for the model\n",
    "    feature_importances = get_feature_importances(model_id)[['feature_name', 'feature_value']]\n",
    "    feature_importances['abs_value'] = abs(feature_importances['feature_value'])\n",
    "    feature_importances = feature_importances.sort_values(by = 'abs_value', ascending = False)\n",
    "    features_to_plot = feature_importances.nlargest(num_features_to_plot,'abs_value').reset_index(drop = True)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    sns.barplot(data = features_to_plot, y = 'feature_name', x = 'feature_value', ax = ax)\n",
    "    ax.set_title(get_model_desc(model_id))\n",
    "\n",
    "\n",
    "\n",
    "def plot_pr_curve(model_id):\n",
    "    \"\"\"Takes in a Model ID and plots the Precision-Recall curve\"\"\"\n",
    "    # get model metrics from database\n",
    "    metrics = run_query(f\"SELECT * from modeling.model_metrics WHERE model_id = '{model_id}';\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    # exclude AUC to get df of only precision recall\n",
    "    pr_metrics =  metrics[(metrics['metric_name']=='precision') | (metrics['metric_name']=='recall')]\n",
    "    pr_metrics =  pr_metrics.reset_index(drop = True)  \n",
    "    sns.lineplot(data = pr_metrics, y ='value', x = 'metric_param', hue = 'metric_name', ax = ax)\n",
    "    ax.set_title(get_model_desc(model_id))\n",
    "    ax.set_xlabel(\"Percent of population labeled 1\")\n",
    "\n",
    "def get_a_metric(model_id, metric_name, metric_param):\n",
    "    df = run_query(f\"\"\"SELECT * FROM modeling.model_metrics \n",
    "                       WHERE model_id = '{model_id}' \n",
    "                           and metric_name = '{metric_name}'\n",
    "                           and metric_param = {metric_param}\"\"\")\n",
    "    print(f\"{metric_name} at {metric_param} = \", df['value'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline model ID\n",
    "baseline_df = run_query(\"\"\"SELECT * \n",
    "          FROM modeling.model_metadata\n",
    "          LEFT JOIN modeling.model_set_metadata using (model_set_id)\n",
    "          WHERE model_type = 'num_cases_baseline'\n",
    "          ORDER BY run_id DESC, val_end_date DESC;\n",
    "           \"\"\")\n",
    "baseline_model_id = baseline_df.loc[0,'model_id']\n",
    "baseline_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the Model ID for the model you're interested in - otherwise this will not work\n",
    "\n",
    "# One of the best performing models\n",
    "model_id = 'c3131f37c72932bda74c61355ba65293'\n",
    "\n",
    "# Baseline model\n",
    "# model_id = baseline_model_id\n",
    "\n",
    "# insert the percentage of population to be labeled as 1 for crosstab calculations. Defaults to 10 if no value is inserted\n",
    "#perc = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision - Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision at 10 and recall at 10 \n",
    "get_a_metric(model_id,'precision', 10)\n",
    "get_a_metric(model_id,'recall', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent of people who return in the bottom 10% (want to be low)\n",
    "metric_param = 10\n",
    "df = run_query(f\"\"\"SELECT * FROM modeling.model_predictions\n",
    "                  WHERE model_id = '{model_id}';\"\"\")\n",
    "\n",
    "val_size = df.shape[0]\n",
    "num_in_bottom_10 = np.floor(val_size * metric_param/100)\n",
    "num_in_bottom_10\n",
    "df.loc[df['rank'] > num_in_bottom_10,'true_label'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_dist(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_dist(model_id,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importances(model_id)\n",
    "plot_top_features(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the variables to split the data by\n",
    "crosstab_features = [\"race\", \"avg_age\", \"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "crosstab_data, column_names = predictions_by_cats(model_id, crosstab_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstabs = crosstab_at_k(crosstab_data, column_names)\n",
    "display(crosstabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias audition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "import aequitas.plot as ap\n",
    "DPI = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define:\n",
    "1) The attributes we want to audit and the reference group for each attribute\n",
    "2) The metrics we are interestid in\n",
    "3) Our disparity tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_and_reference_groups={'sex':'Male', 'race':'White'}\n",
    "attributes_to_audit = list(attributes_and_reference_groups.keys())\n",
    "metrics = ['for', 'fnr']\n",
    "disparity_tolerance = 1.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictions, labels and attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data and get into the correct shape\n",
    "audit_data, column_names = predictions_by_cats(model_id, attributes_to_audit)\n",
    "# assign score of 1 or 1 by rank\n",
    "perc = 10   # percent of defendants that will be labeled as high risk\n",
    "num_labeled_pos = np.ceil(perc * audit_data.shape[0]/100)\n",
    "audit_data['score'] = (audit_data['rank'] < num_labeled_pos).astype(int)\n",
    "\n",
    "# Mapping from hot-encoding to single category columns\n",
    "audit_data.loc[audit_data['sex_f'] == 1, 'sex'] = 'Female'\n",
    "audit_data.loc[audit_data['sex_m'] == 1, 'sex'] = 'Male'\n",
    "audit_data.loc[audit_data['sex_x'] == 1, 'sex'] = 'Trans'\n",
    "audit_data.loc[audit_data['sex_missing'] == 1, 'sex'] = 'Missing'\n",
    "audit_data.loc[audit_data['race_a'] == 1, 'race'] = 'Asian'\n",
    "audit_data.loc[audit_data['race_b'] == 1, 'race'] = 'Black'\n",
    "audit_data.loc[audit_data['race_i'] == 1, 'race'] = 'American_Indian'\n",
    "audit_data.loc[audit_data['race_w'] == 1, 'race'] = 'White'\n",
    "audit_data.loc[audit_data['race_u'] == 1, 'race'] = 'Unknown'\n",
    "audit_data.loc[audit_data['race_missing'] == 1, 'race'] = 'Missing'\n",
    "\n",
    "audit_data = audit_data[[\"score\", \"true_label\", \"sex\", \"race\"]]\n",
    "audit_data = audit_data.rename(columns = {\"true_label\": \"label_value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the audit_data dataframe\n",
    "audit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Aequitas\n",
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "# get_crosstabs returns a dataframe of the group counts and group value bias metrics.\n",
    "xtab, _ = g.get_crosstabs(audit_data, attr_cols=attributes_to_audit)\n",
    "bdf = b.get_disparity_predefined_groups(xtab, original_df=audit_data, ref_groups_dict=attributes_and_reference_groups)\n",
    "\n",
    "bdf = bdf.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Omission Rate (FOR) - Among people that were not classified as high risk, what is the probability of returning with a new case\n",
    "as a function of race / sex?\n",
    "\n",
    "Fase Negative Rate (FNR) - Among people that returned with a new case, what is the probability of not being classified as high risk as a function of race / sex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.disparity(bdf, metrics, 'race', fairness_threshold = disparity_tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.disparity(bdf, metrics, 'sex', fairness_threshold = disparity_tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the underlying data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disparities for all metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf[['attribute_name', 'attribute_value'] + b.list_disparities(bdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "xtab[['attribute_name', 'attribute_value'] + absolute_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab[[col for col in xtab.columns if col not in absolute_metrics]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "tp = xtab['tp'].sum()\n",
    "fp = xtab['fp'].sum()\n",
    "tn = xtab['tn'].sum()\n",
    "fn = xtab['fn'].sum()\n",
    "\n",
    "print(\"Overall FOR: \", fn/(fn+tn))\n",
    "print(\"Overall TPR: \", tp/(tp+fn))\n",
    "print(\"Overall FPR: \", fp/(fp+tn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "167ecea97b6cee78543450b0b9c999fce57adc24f032573ce6fc535d8cc7c5c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
