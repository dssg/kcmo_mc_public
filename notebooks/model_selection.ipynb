{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "Uses model run information saved in database to compare performance of all models, and select top models based on precision at 10% and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions  \n",
    "def run_query(query):\n",
    "    engine = create_engine(\"postgresql:///kcmo-mc\")\n",
    "    db_conn = engine.connect()\n",
    "\n",
    "    with db_conn.begin():\n",
    "        df = pd.read_sql(query, db_conn)\n",
    "    return df\n",
    "\n",
    "def get_model_info(model_id):\n",
    "    query = f\"\"\"SELECT * from modeling.model_metadata\n",
    "            LEFT JOIN  modeling.model_set_metadata \n",
    "            USING (model_set_id)\n",
    "            WHERE model_id = '{model_id}'\n",
    "            ;\"\"\"\n",
    "\n",
    "    df = run_query(query)\n",
    "    # For now return only first row - all roes should contain the same info except for run\n",
    "    return df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_df():\n",
    "    query = f\"\"\"select distinct\n",
    "\tmsm.model_set_id, \n",
    "\tmsm.model_type,\n",
    "\tmodel_hyperparams,\n",
    "\ttemporal_params,\n",
    "\tmd.model_id, \n",
    "\trun_id,\n",
    "\tmd.train_start_date::date,\n",
    "\tmd.train_end_date::date, \n",
    "\tmd.val_start_date::date,\n",
    "\tmd.val_end_date::date,\n",
    "\tmd.val_size,\n",
    "\tmd.val_count_positive,\n",
    "\tDATE(md.started_training_at) as started_training_at,\n",
    "\tmetric_name, \n",
    "\tmetric_param, \n",
    "\tvalue\n",
    "\tfrom modeling.model_set_metadata msm \n",
    "\tleft join modeling.model_metadata md\n",
    "\t\ton msm.model_set_id = md.model_set_id\n",
    "\tleft join modeling.model_metrics mm \n",
    "\t\ton mm.model_id = md.model_id \n",
    "\t-- where metric_name = 'precision' \n",
    "\t-- and metric_param in ('5', '10', '50', '90', '95', '100')\n",
    "\torder by started_training_at, val_end_date::date;\"\"\"\n",
    "\n",
    "    return run_query(query)\n",
    "metrics_df  = get_metrics_df()\n",
    "metrics_df['metric_param'] = metrics_df['metric_param'].fillna('None')\n",
    "metrics_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC over the time splits \n",
    "def audition_plot(metrics_df, metric_name, metric_param):\n",
    "    prec_10 = metrics_df[(metrics_df['metric_name']==metric_name) & (metrics_df['metric_param']==metric_param)]\n",
    "    fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "    ax = sns.lineplot(x = 'val_end_date', y = 'value', data = prec_10, hue = 'model_type', style = 'model_set_id', ax=ax)\n",
    "    ax.set_title(metric_name + \"@\" + str(metric_param))\n",
    "    ax.set_xlabel('Validation End Date')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.legend(bbox_to_anchor=(1.3, .6), loc='right')\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "audition_plot(metrics_df, metric_name = 'AUC', metric_param = 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision over splits \n",
    "audition_plot(metrics_df, metric_name = 'precision', metric_param = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top models by precision at 10%\n",
    "def get_top_models(metric_name = 'precision', metric_param = 10, top_n = 3):\n",
    "    # Calculate mean value for each metric \n",
    "    metrics_df['model_hyperparams_str'] = metrics_df['model_hyperparams'].astype(str)\n",
    "    set_metrics_df = metrics_df.groupby(['model_set_id','model_type','model_hyperparams_str', 'metric_name','metric_param']).mean()[['value']]\n",
    "    set_metrics_df = set_metrics_df.reset_index(drop=False)\n",
    "    # Narrow down to metric of interest (e.g. precision @ 10, top 3)\n",
    "    set_metric_df = set_metrics_df[(set_metrics_df['metric_name'] == metric_name) & (set_metrics_df['metric_param'] == metric_param)]\n",
    "    set_metric_df = set_metric_df.nlargest(n = top_n, columns='value', keep='first')\n",
    "    # Left join all models in each model_set (model for each time split)\n",
    "    model_metadata = run_query(\"SELECT * from modeling.model_metadata\")\n",
    "    model_metric_df = pd.merge(set_metric_df, model_metadata, on='model_set_id', how='left')\n",
    "    # Select the models only with most recent validation scheme\n",
    "    last_val_end_date = max(model_metric_df['val_end_date'])\n",
    "    top_models = model_metric_df[model_metric_df['val_end_date'] == last_val_end_date]\n",
    "    top_model_ids = list(top_models['model_id'])\n",
    "    display(top_models[['model_id','model_set_id', 'model_type','model_hyperparams_str','metric_name','metric_param','value']])\n",
    "    return (top_model_ids, top_models)\n",
    "\n",
    "top_model_ids, top_models = get_top_models(metric_name = 'precision', metric_param = 10, top_n = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top models by AUC \n",
    "top_model_ids, top_models = get_top_models(metric_name = 'AUC', metric_param = 'None', top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get all models corresponding to a model_set_id (in this case, the first one)\n",
    "model_set_id = top_models.loc[0,'model_set_id']\n",
    "all_models_over_time = run_query(f\"SELECT * FROM modeling.model_metadata where model_set_id = '{model_set_id}';\")\n",
    "all_models_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Base Rate over time for precision understanding \n",
    "top_model_set_id = top_models.loc[0,'model_set_id']\n",
    "one_model = metrics_df[(metrics_df['model_set_id'] == top_model_set_id)& (metrics_df['metric_name'] == 'AUC')].copy()\n",
    "one_model['returns_base_rate'] = one_model['val_count_positive']/one_model['val_size']\n",
    "fig, ax = plt.subplots(1, figsize=(10,7))\n",
    "sns.lineplot(x = one_model['val_end_date'], y = one_model['returns_base_rate'], ax=ax)\n",
    "ax.set_title(\"Base rate of returns within one year for individuals put on SIS probation over time\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "167ecea97b6cee78543450b0b9c999fce57adc24f032573ce6fc535d8cc7c5c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
